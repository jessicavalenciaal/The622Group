---
title: "622 HW2 GROUP"
author: "Ruiling Kang, Sihle Khanyile, Suzy McTaggart, Jessica Valencia"
date: "2023-03-04"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 2: Using the Reddit API and Sentiment Analysis of Reddit Posts

For this assignment, the Reddit API was used to scrape information about Michigan State University, including posts about the recent shooting at the school.  The corpus of posts was collected by using the RedditExtractoR package in R.

```{r load_package}
#install.packages("RedditExtractoR")
#devtools::install_github('ivan-rivera/RedditExtractor')
library(RedditExtractoR)
```

### Scraping Posts

We used the built-in find_thread_urls feature with the keyword "MSU shooting" to locate the URLs to Reddit threads of interest (in other words, threads that contained the string "MSU shooting"). Data was colled from the "front page" of Reddit, rather than any specific subreddit in this first step.

```{r find_thread_url_msu}
#msu_urls = find_thread_urls(keywords='MSU Shooting', period='month')
#dim(msu_urls)
#head(msu_urls)
```

The URLs were then used to extract the content of the threads/posts as well as metadata describing each thread.  This process was repeated over several days to "listen" to the corpus of posts about Michigan State University over time.

```{r get_thread_content}
#msu_content = get_thread_content(msu_urls$url[1:25])

#names(msu_content)

#dim(msu_content$threads)
#head(msu_content$threads)
```
```{r}
#dim(msu_content$comments)
#head(msu_content$comments)
```

```{r}
#write.csv(msu_urls, "C:\\Users\\valen\\Downloads\\msudata.csv", row.names=FALSE)
```

### Merging Data

Each day a csv was collected and at the end of the data collection window (6 days) the information was merged to a single dataset.  

**From the assignment description: The report each group is asked to turn in for this assignment should specify how the keywords and/or subreddits used for collecting the posts were selected. The keywords and/or subreddits should be listed, along with the dates when listening occurred. If keywords and/or subreddits were changed during the data collection process, please note the changes that were made, the reason(s) for them and when they occurred. Listening ideally will occur continuously, but if there are gaps, the time(s) of these gaps should be noted. **

```{r}
library(readr)
#msudata1 <- read_csv("C:/Users/valen/Downloads/msudata (1).csv")
#View(msudata_1_)

#msudata2 <- read_csv("C:/Users/valen/Downloads/msudata2.csv")
#View(msudata2)

#msudata3 <- read_csv("C:/Users/valen/Downloads/msudata3.csv")
#View(msudata3)

#msudata4 <- read_csv("C:/Users/valen/Downloads/msudata4.csv")
#View(msudata4)

#msudata5 <- read_csv("C:/Users/valen/Downloads/msudata5.csv")
#View(msudata5)

#msudata6 <- read_csv("C:/Users/valen/Downloads/msudata6.csv")
#View(msudata6)

#full_data<- rbind(msudata1,msudata2,msudata3,msudata4,msudata5,msudata6)
#View(full_data)
#write.csv(full_data, "C:\\Users\\valen\\Downloads\\msufulldata.csv", #row.names=FALSE)
```

### Accessing the Full Dataset

The full corpus of Reddit threads scraped and the code for this analysis was loaded to a shared github repository, which each member of the analysis team (JV, SK, RK, SM) collaborative updated and analyzed.

```{r}
library(readr)
#where you will bring in the data
msufulldata <- read.csv('https://raw.githubusercontent.com/jessicavalenciaal/The622Group/main/msufulldata.csv',sep=',')
originaldim<-dim(msufulldata)
originaldim
View(msufulldata)
head(msufulldata)
```

### Cleaning the Dataset

The timestamp variable is given as a UNIX timestamp, the number of seconds from 1/1/70. We first convert this variable into a date and time. (Reference: Assignment RMarkdown, Canvas)

```{r datetime}
library(lubridate)
msufulldata$datetime = as_datetime(msufulldata$timestamp)
head(msufulldata[,c('date_utc', 'datetime')])
```

Further we need to verify that information collected truly relate to the topic of interest - Michigan State University. A sample of 15 comments were used for preliminary exploratory analysis of the corpus to verify that the collected data are generally aligned with the specified topic.

```{r}
#print(msufulldata[sample(1:nrow(msufulldata), 15),])
```

This sample demonstrates that there are comments/threads that were scraped that are irrelevant to our topic/analysis and should be removed from the corpus.  We can subset the data by limiting comments to those that include specified phases in the post title and/or body.  

We begin with limiting to any posts that contain the phrases "Michigan State University", "MSU", "MSU Shooting", "Michigan State", and "Spartan".

```{r}
patterns = 'Michigan State University|MSU|MSU Shooting|Michigan State|Spartan'
subset_msu_posts = msufulldata[grepl(patterns, msufulldata$title, ignore.case=TRUE) |
                                         grepl(patterns, msufulldata$text, ignore.case=TRUE),]
subsetdim<-dim(subset_msu_posts)
subsetdim
head(subset_msu_posts)
View(subset_msu_posts)
```

Using this criteria to subset the scraped Reddit corpus reduces our observations from `r originaldim[1]` to `r subsetdim[1]`.



