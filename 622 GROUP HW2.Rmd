---
title: "622 HW2 GROUP"
author: "Ruiling Kang, Sihle Khanyile, Suzy McTaggart, Jessica Valencia"
date: "2023-03-04"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 2: Using the Reddit API and Sentiment Analysis of Reddit Posts

For this assignment, the Reddit API was used to scrape information about Michigan State University, including posts about the recent shooting at the school.  The corpus of posts was collected by using the RedditExtractoR package in R.

```{r load_package}
#install.packages("RedditExtractoR")
#devtools::install_github('ivan-rivera/RedditExtractor')
library(RedditExtractoR)
```

### Scraping Posts

We used the built-in find_thread_urls feature with the keyword "MSU shooting" to locate the URLs to Reddit threads of interest (in other words, threads that contained the string "MSU shooting"). Data was colled from the "front page" of Reddit, rather than any specific subreddit in this first step.

```{r find_thread_url_msu}
#msu_urls = find_thread_urls(keywords='MSU Shooting', period='month')
#dim(msu_urls)
#head(msu_urls)
```

The URLs were then used to extract the content of the threads/posts as well as metadata describing each thread.  This process was repeated over several days to "listen" to the corpus of posts about Michigan State University over time.

```{r get_thread_content}
#msu_content = get_thread_content(msu_urls$url[1:25])

#names(msu_content)

#dim(msu_content$threads)
#head(msu_content$threads)
```
```{r}
#dim(msu_content$comments)
#head(msu_content$comments)
```

```{r}
#write.csv(msu_urls, "C:\\Users\\valen\\Downloads\\msudata.csv", row.names=FALSE)
```

### Merging Data

Each day a csv was collected and at the end of the data collection window (6 days) the information was merged to a single dataset.  

**From the assignment description: The report each group is asked to turn in for this assignment should specify how the keywords and/or subreddits used for collecting the posts were selected. The keywords and/or subreddits should be listed, along with the dates when listening occurred. If keywords and/or subreddits were changed during the data collection process, please note the changes that were made, the reason(s) for them and when they occurred. Listening ideally will occur continuously, but if there are gaps, the time(s) of these gaps should be noted. **

```{r}
library(readr)
#msudata1 <- read_csv("C:/Users/valen/Downloads/msudata (1).csv")
#View(msudata_1_)

#msudata2 <- read_csv("C:/Users/valen/Downloads/msudata2.csv")
#View(msudata2)

#msudata3 <- read_csv("C:/Users/valen/Downloads/msudata3.csv")
#View(msudata3)

#msudata4 <- read_csv("C:/Users/valen/Downloads/msudata4.csv")
#View(msudata4)

#msudata5 <- read_csv("C:/Users/valen/Downloads/msudata5.csv")
#View(msudata5)

#msudata6 <- read_csv("C:/Users/valen/Downloads/msudata6.csv")
#View(msudata6)

#full_data<- rbind(msudata1,msudata2,msudata3,msudata4,msudata5,msudata6)
#View(full_data)
#write.csv(full_data, "C:\\Users\\valen\\Downloads\\msufulldata.csv", #row.names=FALSE)
```

### Accessing the Full Dataset

The full corpus of Reddit threads scraped and the code for this analysis was loaded to a shared github repository, which each member of the analysis team (JV, SK, RK, SM) collaborative updated and analyzed.

```{r}
library(readr)
#where you will bring in the data
msufulldata <- read.csv('https://raw.githubusercontent.com/jessicavalenciaal/The622Group/main/msufulldata.csv',sep=',')
originaldim<-dim(msufulldata)
originaldim
#View(msufulldata)
head(msufulldata)
```

### Cleaning the Dataset

The timestamp variable is given as a UNIX timestamp, the number of seconds from 1/1/70. We first convert this variable into a date and time. (Reference: Assignment RMarkdown, Canvas)

```{r datetime, message=FALSE}
library(lubridate)
msufulldata$datetime = as_datetime(msufulldata$timestamp)
head(msufulldata[,c('date_utc', 'datetime')])
```

Further we need to verify that information collected truly relate to the topic of interest - Michigan State University. A sample of 15 comments were used for preliminary exploratory analysis of the corpus to verify that the collected data are generally aligned with the specified topic.

```{r}
#print(msufulldata[sample(1:nrow(msufulldata), 15),])
```

This sample demonstrates that there are comments/threads that were scraped that are irrelevant to our topic/analysis and should be removed from the corpus.  We can subset the data by limiting comments to those that include specified phases in the post title and/or body.  

We begin with limiting to any posts that contain the phrases "Michigan State University", "MSU", "MSU Shooting", "Michigan State", and "Spartan".

```{r}
patterns = 'Michigan State University|MSU|MSU Shooting|Michigan State|Spartan'
subset_msu = msufulldata[grepl(patterns, msufulldata$title, ignore.case=TRUE) |
                                         grepl(patterns, msufulldata$text, ignore.case=TRUE),]
subsetdim<-dim(subset_msu)
subsetdim
head(subset_msu)
#View(subset_msu)
```

Using this criteria to subset the scraped Reddit corpus reduces our observations from `r originaldim[1]` to `r subsetdim[1]`.

Given the notable reduction in the sample (reduced by approximately half) a quick manual review was conducted to ensure there were not commonly used phases that should be added to the inclusion criteria.  No additional phrases were identified that would notably improve coverage of the topic in the corpus.

### Frequent Terms

To continue the exploratory analysis of the dataset, we used the qdap package to identify the most frequently used phrases in our updated (subsetted) corpus.

```{r, warning=FALSE, message=FALSE}
# Workaround needed to prevent RStudio from crashing
replacement <- function(category = "LC_ALL") {
  
  if (identical(category, "LC_MESSAGES"))
    return("")
  
  category <- match(category, .LC.categories)
  if (is.na(category)) 
    stop("invalid 'category' argument")
  .Internal(Sys.getlocale(category))
  
}

base <- asNamespace("base")
environment(replacement) <- base
unlockBinding("Sys.getlocale", base)
assign("Sys.getlocale", replacement, envir = base)
lockBinding("Sys.getlocale", base)

library(rJava)
rJava::.jinit()

# Load qdap package
library(qdap)

# concatenate post title and text
subset_msu$title_text = paste(subset_msu$title, subset_msu$text)
frequent_terms = freq_terms(subset_msu$title_text, 30)
plot(frequent_terms)
```

As shown in the plot above, many of the frequent words are "stop words" that are not relevant to understanding the content of the post.  We do, however, see the expected and very relevant words "shooting" and "msu".

To make the review of frequent words more meaningful, we filter out the stop words and re-compute the most frequently used words in the corpus.

```{r}
msustopwords = subset_msu$title_text %>% iconv ("latin1", "ASCII", sub ="") %>% scrubber () %sw%
qdapDictionaries :: Top200Words

frequent_terms = freq_terms(msustopwords, 30)
plot(frequent_terms)
```

Without stop words, additional themes begin to emerge such as "gun" and "game".  These two words indicate that there may be multiple popular topics about Michigan State University being discussed concurrently on Reddit.  There remain words that may be non-informative, such as www, govmailto, b, x, etc.  However, these together may indicate a trend in the posts urging readers to reach out to government representatives - which is an important aspect of the ongoing trend during the listening period.

### Exploratory Analysis

Before doing formal statistical analyses on the scraped and subsetted data, we may like to know more about where it came from specifically within the Reddit site.

For instance, we can review the subreddits that contain the posts of interest.

```{r, message=FALSE}
subredditfreq<-table(subset_msu$subreddit)
library(dplyr)

subfreqordered <- subredditfreq %>% as.data.frame() %>% arrange(desc(Freq))
subfreqordered[1:10,]
```

The top 10 subreddits that contained posts within our corpus as shown above (including "masskillers", "gunpolitics", and "GunsAreCool") support the prevalence of the MSU shooting within the scraped data.

We can also review when the posts within our corpus were added to Reddit.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(subset_msu, aes(x=datetime)) +
  geom_histogram() +
  xlab('Date') + ylab('Count') + ggtitle('Frequency of MSU Shooting Posts within the last month')
```
The shooting at Michigan State University occurred on Monday, February 13.  The plot of posts shows the highest frequency of selected posts beginning that day and within the 3 days that followed.

Reddit allows for users to comment on posts, which may provide insights into the overall engagement with posts related to the MSU Shooting.  

```{r}
comsum<-summary(subset_msu$comments)
comsum
```
The median number of comments on the posts within the corpus is `r comsum[3]` which is relatively small, but there is a wide spread of engagement across different posts ranging from `r comsum[1]` to `r comsum[6]`.

### Sentiment Analysis

Sentiment analysis allows us to review the overall positive or negative tone of posts within our corpus using 5 different dictionaries:

* GI: Harvard-IV General Inquirer dictionary
* HE: Henry's finance specific dictionary
* LM: Loughran-McDonald financial dictionary
* QDAP
* Lexicoder

```{r, message=FALSE}
library(SentimentAnalysis)

#DictionaryGI$positive[1:100]
#DictionaryGI$negative[1:100]

library(quanteda)

#data_dictionary_LSD2015$negative[1:50]
#data_dictionary_LSD2015$positive[1:50]
#data_dictionary_LSD2015$neg_positive[1:50]
#data_dictionary_LSD2015$neg_negative[1:50]

sentiments = analyzeSentiment(iconv(as.character(subset_msu$title_text), to='UTF-8'))
head(sentiments)
```


```{r}
tokenized = tokens_lookup(tokens(subset_msu$title_text), dictionary = data_dictionary_LSD2015, exclusive=FALSE)
sentiments$LCpos = sapply(tokenized, function(x)sum(x=='POSITIVE')-sum(x=='NEG_POSITIVE') + sum(x =='NEG_NEGATIVE'))
sentiments$LCneg = sapply(tokenized, function(x)sum(x=='NEGATIVE')-sum(x=='NEG_NEGATIVE') + sum(x=='NEG_POSITIVE'))
sentiments$LC = (sentiments$LCpos - sentiments$LCneg)/sentiments$WordCount
```



```{r, cache=TRUE}
library(vader)

vader_scores = vader_df(subset_msu$title_text)
sentiments$Vader = vader_scores$compound

library(GGally)

with(sentiments, 
     ggpairs(data.frame(SentimentGI, SentimentHE, SentimentLM, SentimentQDAP, LC, Vader)))
```


```{r}
all_subset_msu_data = cbind(subset_msu, sentiments)
ggplot(all_subset_msu_data, aes(x=as.Date(date_utc), y=Vader)) +
  geom_point() + geom_smooth()
```


Difference in sentiment between MSU subreddit and GunsAreCool subreddit
```{r}
table(all_subset_msu_data$subreddit)

par(mfrow=c(1,2))
hist(all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='msu'], main='msu', xlab='Vader')
hist(all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='GunsAreCool'], main='GunsAreCool', xlab='Vader')

t.test(all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='msu'],
       all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='GunsAreCool'])
```