---
title: "622 HW2 GROUP"
author: "Ruiling Kang, Sihle Khanyile, Suzy McTaggart, Jessica Valencia"
date: "2023-03-04"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 2: Using the Reddit API and Sentiment Analysis of Reddit Posts

For this assignment, the Reddit API was used to scrape information about Michigan State University, including posts about the recent shooting at the school.  The corpus of posts was collected by using the RedditExtractoR package in R.

```{r load_package}
#install.packages("RedditExtractoR")
#devtools::install_github('ivan-rivera/RedditExtractor')
library(RedditExtractoR)
```

### Introduction

For this assignment we decided to look at Michigan State University(MSU) as our topic of interest on Reddit. We found that the MSU shooting was a significant event that was a common topic of discussion and we wanted to see the prevalence of this event on Reddit compared to other events regarding Michigan State University. We decided to scrape data using various different variations of Michigan State University. After scraping day by day for several days, our raw data totaled to 1224 posts. We then cleaned the data which resulted in a data frame of 624 posts and then conducted exploratory analysis on this data. After exploring the data and familiarizing ourselves with the data we decided to look at it more closely through a sentiment analysis using 5 dictionary based methods: Harvard-IV General Inquirer dictionary, Henry’s finance specific dictionary, Loughran-McDonald financial dictionary, QDAP, Lexicoderand, and one-rule based method. 


### Scraping Posts

We used the built-in find_thread_urls feature with the keyword "MSU shooting" to locate the URLs to Reddit threads of interest (in other words, threads that contained the string "MSU shooting"). Data was colled from the "front page" of Reddit, rather than any specific subreddit in this first step.

```{r find_thread_url_msu}
#msu_urls = find_thread_urls(keywords='MSU Shooting', period='month')
#dim(msu_urls)
#head(msu_urls)
```

The URLs were then used to extract the content of the threads/posts as well as metadata describing each thread.  This process was repeated over several days to "listen" to the corpus of posts about Michigan State University over time.

When deciding on how we wanted to go about scrapping the data, we found that collecting over multiple days was the best method considering there is a cap to how much data you are able to scrape per day. We scraped data over the course of 6 days, March 3rd to March 8th. Data was initially scraped with period being set as ‘day’, meaning the data being scraped was data from that day. However, we found that a sufficient amount of data was not being scraped through this method on the first day of scraping. The first scrape done using ‘day’ as the period scraped under 100 posts. Therefore, we decided to do a scrape using ‘month’ as the period in the beginning of scraping, that way we scraped data over the past month as well. However, we still scraped by day moving forward. We also used multiple different variations of Michigan State University in order to make sure we had a sufficient amount of relevant data. The words used to search the proper data were “MSU”, “msu”,”Michigan State University”, “michigan shooting”, “msu shooting”, and “MSU shooting”. 


```{r get_thread_content}
#msu_content = get_thread_content(msu_urls$url[1:25])

#names(msu_content)

#dim(msu_content$threads)
#head(msu_content$threads)
```
```{r}
#dim(msu_content$comments)
#head(msu_content$comments)
```

```{r}
#write.csv(msu_urls, "C:\\Users\\valen\\Downloads\\msudata.csv", row.names=FALSE)
```

### Merging Data and Accessing the Full Dataset

Each day a csv was collected and at the end of the data collection window (6 days) the information was merged to a single dataset.  

```{r}
library(readr)
#msudata1 <- read_csv("C:/Users/valen/Downloads/msudata (1).csv")
#View(msudata_1_)

#msudata2 <- read_csv("C:/Users/valen/Downloads/msudata2.csv")
#View(msudata2)

#msudata3 <- read_csv("C:/Users/valen/Downloads/msudata3.csv")
#View(msudata3)

#msudata4 <- read_csv("C:/Users/valen/Downloads/msudata4.csv")
#View(msudata4)

#msudata5 <- read_csv("C:/Users/valen/Downloads/msudata5.csv")
#View(msudata5)

#msudata6 <- read_csv("C:/Users/valen/Downloads/msudata6.csv")
#View(msudata6)

#full_data<- rbind(msudata1,msudata2,msudata3,msudata4,msudata5,msudata6)
#View(full_data)
#write.csv(full_data, "C:\\Users\\valen\\Downloads\\msufulldata.csv", #row.names=FALSE)
```

The full corpus of Reddit threads scraped and the code for this analysis was loaded to a shared github repository, which each member of the analysis team (JV, SK, RK, SM) collaborative updated and analyzed.

```{r}
library(readr)
#where you will bring in the data
msufulldata <- read.csv('https://raw.githubusercontent.com/jessicavalenciaal/The622Group/main/msufulldata.csv',sep=',')
originaldim<-dim(msufulldata)
originaldim
#View(msufulldata)
head(msufulldata)
```

### Cleaning the Dataset

The first step in conducting a reliable analysis is ensuring that the data is sufficiently cleaned to avoid construct-irrelevant effects.  The data cleaning step includes both data formatting processes and removal of irrelevant data.
First, we notice that the timestamp variable is given as a UNIX timestamp, the number of seconds from 1/1/70.  We convert this variable into a date and time to enable analyses of when the posts occurred. (Reference: Assignment RMarkdown, Canvas)

```{r datetime, message=FALSE}
library(lubridate)
msufulldata$datetime = as_datetime(msufulldata$timestamp)
head(msufulldata[,c('date_utc', 'datetime')])
```

Further we need to verify that information collected truly relate to the topic of interest - Michigan State University. A sample of 15 comments were used for preliminary exploratory analysis of the corpus to verify that the collected data are generally aligned with the specified topic.

```{r, results=FALSE}
#print(msufulldata[sample(1:nrow(msufulldata), 15),])
```

This sample demonstrates that there are comments/threads that were scraped that are irrelevant to our topic/analysis and should be removed from the corpus. We can subset the data by limiting comments to those that include specified phases in the post title and/or body. We begin with limiting to any posts that contain the phrases “Michigan State University”, “MSU”, “MSU Shooting”, “Michigan State”, and “Spartan”.

```{r}
patterns = 'Michigan State University|MSU|MSU Shooting|Michigan State|Spartan'
subset_msu = msufulldata[grepl(patterns, msufulldata$title, ignore.case=TRUE) |
                                         grepl(patterns, msufulldata$text, ignore.case=TRUE),]
subsetdim<-dim(subset_msu)
subsetdim
#head(subset_msu)
#View(subset_msu)
```

Using this criteria to subset the scraped Reddit corpus reduces our observations from `r originaldim[1]` to `r subsetdim[1]`.

Given the notable reduction in the sample (reduced by approximately half) a manual review was conducted to ensure there were not commonly used phases that should be added to the inclusion criteria. The reviewer (SM) looked for phrases that may have been missed for inclusion or unexpected items that would be included but should not be.  The term “Michigan” was inspected closely but was ultimately determined that meaningful inclusions with the term Michigan were already caught by the other specified key words.

No additional phrases were identified that would notably improve coverage of the topic in the corpus.

### Exploratory Analysis

#### Frequent Terms

Given the importance of term inclusion for this analysis, it is important that we explore and begin to understand the phrases most often occurring in the corpus.  To that end, we used the qdap package to identify the most frequently used phrases in our updated (subsetted) corpus.

```{r, warning=FALSE, message=FALSE}
# Workaround needed to prevent RStudio from crashing
replacement <- function(category = "LC_ALL") {
  
  if (identical(category, "LC_MESSAGES"))
    return("")
  
  category <- match(category, .LC.categories)
  if (is.na(category)) 
    stop("invalid 'category' argument")
  .Internal(Sys.getlocale(category))
  
}

base <- asNamespace("base")
environment(replacement) <- base
unlockBinding("Sys.getlocale", base)
assign("Sys.getlocale", replacement, envir = base)
lockBinding("Sys.getlocale", base)

library(rJava)
rJava::.jinit()

# Load qdap package
library(qdap)

# concatenate post title and text
subset_msu$title_text = paste(subset_msu$title, subset_msu$text)
frequent_terms = freq_terms(subset_msu$title_text, 30)
plot(frequent_terms)
```

As shown in the plot above, many of the frequent words are “stop words” that are not relevant to understanding the content of the post. We do, however, see the expected and very relevant words “shooting” and “msu”.
To make the review of frequent words more meaningful, we filter out the stop words and re-compute the most frequently used words in the corpus.

```{r, message=FALSE, warning=FALSE}
msustopwords = subset_msu$title_text %>% iconv ("latin1", "ASCII", sub ="") %>% scrubber () %sw%
qdapDictionaries :: Top200Words

frequent_terms = freq_terms(msustopwords, 30)
plot(frequent_terms)
```

Without stop words, additional themes begin to emerge such as “gun” and “game”. These two words indicate that there may be multiple popular topics about Michigan State University being discussed concurrently on Reddit. There remain words that may be non-informative, such as www, govmailto, b, x, etc. However, these together may indicate a trend in the posts urging readers to reach out to government representatives - which is an important aspect of the ongoing trend during the listening period.

#### Frequency and Location of Posts and Comments

Before doing formal statistical analyses on the scraped and subsetted data, we may like to know more about where it came from specifically within the Reddit site.

For instance, we can review the subreddits that contain the posts of interest.

```{r, message=FALSE}
subredditfreq<-table(subset_msu$subreddit)
library(dplyr)

subfreqordered <- subredditfreq %>% as.data.frame() %>% arrange(desc(Freq))
subfreqordered[1:10,]
```

The top 10 subreddits that contained posts within our corpus as shown above (including “masskillers”, “gunpolitics”, and “GunsAreCool”) support the prevalence of the MSU shooting within the scraped data.

We can also review when the posts within our corpus were added to Reddit.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(subset_msu, aes(x=datetime)) +
  geom_histogram() +
  xlab('Date') + ylab('Count') + ggtitle('Frequency of MSU Shooting Posts within the last month')
```
The shooting at Michigan State University occurred on Monday, February 13. The plot of posts shows the highest frequency of selected posts beginning that day and within the 3 days that followed.

Reddit allows for users to comment on posts, which may provide insights into the overall engagement with posts related to the MSU Shooting.  


```{r}
comsum<-summary(subset_msu$comments)
comsum
```
The median number of comments on the posts within the corpus is `r comsum[3]` which is relatively small, but there is a wide spread of engagement across different posts ranging from `r comsum[1]` to `r comsum[6]`.

### Sentiment Analysis

For this analysis, we use 4 dictionary-based methods – specifically we used the following dictionaries to analyzed the sentiment of posts:

* GI: Harvard-IV General Inquirer dictionary – a widely used dictionary in sentiment analysis that contain approximately 4,500 positively and negatively classified words along with categories for certainty, activity, and pleasantness
* HE: Henry’s finance specific dictionary – a dictionary tailored toward the financial domain containing approximately 2,400 words
* LM: Loughran-McDonald financial dictionary – another dictionary tailored toward the financial domain that includes broader categories than HE including uncertainty, litigiousness, and modalities
* QDAP – a combination of NLP techniques and machine learning algorithms that categorizes words into 6 categories: positive, negative, anger, fear, sadness, and surprise
* Lexicoder – a dictionary designed for political communication and social media data with categories such as positive, negative, neutral, power, risk, and morality
 
We would anticipate that HE and LM (the financially-focused dictionaries) may not be as relevant to our analysis, but include them for comprehensiveness.


```{r, message=FALSE}
library(SentimentAnalysis)

#DictionaryGI$positive[1:100]
#DictionaryGI$negative[1:100]

library(quanteda)

#data_dictionary_LSD2015$negative[1:50]
#data_dictionary_LSD2015$positive[1:50]
#data_dictionary_LSD2015$neg_positive[1:50]
#data_dictionary_LSD2015$neg_negative[1:50]

sentiments = analyzeSentiment(iconv(as.character(subset_msu$title_text), to='UTF-8'))
head(sentiments)
```

For each dictionary method, we have the number of non-stop words in the title and post, a measure of negative words within the observation (NegativityXX), a measure of positive words within the observation (PositivityXX), and a measure of the overall sentiment (SentimentXX).

Within the first six observations we see variation within the different posts and within the ratings of the different methods.  We can explore these differences by looking at the correlation between the different methods.

```{r}
tokenized = tokens_lookup(tokens(subset_msu$title_text), dictionary = data_dictionary_LSD2015, exclusive=FALSE)
sentiments$LCpos = sapply(tokenized, function(x)sum(x=='POSITIVE')-sum(x=='NEG_POSITIVE') + sum(x =='NEG_NEGATIVE'))
sentiments$LCneg = sapply(tokenized, function(x)sum(x=='NEGATIVE')-sum(x=='NEG_NEGATIVE') + sum(x=='NEG_POSITIVE'))
sentiments$LC = (sentiments$LCpos - sentiments$LCneg)/sentiments$WordCount
```

```{r, cache=TRUE}
library(vader)

vader_scores = vader_df(subset_msu$title_text)
sentiments$Vader = vader_scores$compound

library(GGally)

with(sentiments, 
     ggpairs(data.frame(SentimentGI, SentimentHE, SentimentLM, SentimentQDAP, LC, Vader)))
```

As expected, we notice smaller correlations between the financially-focused dictionaries with the other methods.  The QDAP, Lexicoder, and Vader (rule-based method) are the more highly correlated methods.  Again, this is not a surprising finding given that these methods are most aligned with the kind of data we are analyzing (social media data).  This does bring out an interesting point for sentiment analysis though – the alignment between dictionary and analysis is an important consideration for accuracy and confidence with the resulting findings.

When comparing Vader to other sentiment analysis tools using  the generalized pairs plot we note that vader is significantly correlated to dictionary methods such as General Inquirer (0.5644)  and  Lexicoder (0.639). 


#### Rule-Based Method
Our rule-based method uses the Vader (Valence Aware Dictionary and sEntiment Reasoner) package in R.  Vader is a sentiment lexicon with over 9,000 features ranked by people, not machine learning models, on a scale from -4 to +4 from most negative to most positive.  The average of the ratings were collected, neutral words removed, and a dictionary was generated with over 7,000 elements.  Additional rules are implemented in addition to the collected dictionary phrases that accounts for negations (e.g, the “not” in “not great”) which changes the classification of the sentiment.  Because these rules add nuance, we would expect this rule-based method to be the most accurate of the 5 implemented within this analysis.
 
Through the implementation of the Vader package, we see a wide spread of sentiments within the posts scraped from Reddit.

```{r}
all_subset_msu_data = cbind(subset_msu, sentiments)
ggplot(all_subset_msu_data, aes(x=as.Date(date_utc), y=Vader)) +
  geom_point() + geom_smooth()
```

The plot above of the trend in sentiment over time using a loess smoothing function appears to be generally flat at 0.  This does *not* mean that the posts on the subject were neutral – in fact, the opposite is true.  There were, and continue to be, posts on the extremes of sentiment range.  Because of this generally uniform distribution of sentiment we have to be careful in interpreting the trends to avoid masking important findings through averaging, etc.
To begin quantifying this variance, we can look at whether posts in different subreddits have different overall sentiment.

The posts in our corpus are within the following subreddits:

```{r}
table(all_subset_msu_data$subreddit)

```

##### Comparison of Sentiment Across Subreddits
Given the noted trend of varying sentiments between Reddit posts about the MSU Shooting, we selected 2 subreddits to compare, specifically the “msu” and the “GunsAreCool” subreddits.

```{r}
par(mfrow=c(1,2))
hist(all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='msu'], main='msu', xlab='Vader')
hist(all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='GunsAreCool'], main='GunsAreCool', xlab='Vader')

```

As evident in the histograms above, posts on the msu subreddit have a wider variance, but trend more toward positive sentiment, whereas posts in the GunsAreCool subreddit are primarily negative sentiment.  This aligns with intuition about the subject of the posts and potential users expected to be contributing to those subreddits.
We verify this result formally with a t-test of the sentiments of the posts within each subreddt.

```{r}
t.test(all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='msu'],
       all_subset_msu_data$Vader[all_subset_msu_data$subreddit=='GunsAreCool'])
```

We find convincing evidence (p < 0.003) that the sentiments between these 2 subreddits are significantly different.

### 3 Additional Questions about the Corpus

1) Does post length correlate with vader sentiment scores?
```{r}
corone<-cor(sentiments$WordCount,sentiments$Vader)
corone
```

ANSWER: There is moderate correlation between the length of the Vader sentiments score and the length of the post title and body (`r corone`)



2) Does the sentiment of a post correlate with the number of comments it will receive?  Does the intensity (regardless if positive or negative sentiment) correlate with the number of comments a post will receive?
```{r, echo=FALSE}
#head(all_subset_msu_data)
```

```{r}
#correlation between vader score and number of comments
cortwo<-cor(all_subset_msu_data$Vader,all_subset_msu_data$comments)
cortwo

#create new variable which is absolute value of vader score to indicate the intensity of the vader score regardless if positive or negative - will use to see whether the more positive or negative a post is will correlate with number of comments
all_subset_msu_data$VaderIntesity<-abs(all_subset_msu_data$Vader)
#head(all_subset_msu_data)

#correlation between vader score intensity and number of comments
corthree<-cor(all_subset_msu_data$VaderIntesity,all_subset_msu_data$comments)
corthree

```

ANSWER: There is weak correlation (`r cortwo`) between the Vader sentiment and number of comments.  When expanded further to consider just the intensity of the sentiment score (i.e., how strong the sentiment was regardless of whether positive or negative) the correlation is weaker (`r corthree`).  This seems to indicate that despite relatively weak correlation the direction of the sentiment does contribute to the effect.


3) Is there a difference between the number of comments on positive or negative sentiment thread (as a proxy for engagement)?

Including all posts:

```{r}
library(sjmisc)

#create new variable which is dichomotized to positive vader score and negative vader 
all_subset_msu_data$Vaderposneg<-dicho(all_subset_msu_data$Vader,dich.by=0)
#head(all_subset_msu_data)

par(mfrow=c(1,2))
hist(all_subset_msu_data$comments[all_subset_msu_data$Vaderposneg=='0'], main='Negative Sentiment', xlab='Number of Comments')
hist(all_subset_msu_data$comments[all_subset_msu_data$Vaderposneg=='1'], main='Positive Sentiment', xlab='Number of Comments')

t.test(all_subset_msu_data$comments[all_subset_msu_data$Vaderposneg=='0'],
       all_subset_msu_data$comments[all_subset_msu_data$Vaderposneg=='1'])

```

ANSWER: No, there is not a significant difference between the number of comments on posts with positive sentiment compared to those with negative sentiment.

We expanded further to exclude posts with fewer than 100 comments to remove posts that may not have been viewed often or that overall lacked engagement.  The same analysis was conducted to determine if positive posts received the same number of comments.

```{r}

# Considering only observations that had at least 100 comment

subset_msu_withcomments<- all_subset_msu_data[which(all_subset_msu_data$comments > 100), ]

dim(subset_msu_withcomments)
head(subset_msu_withcomments)

par(mfrow=c(1,2))
hist(subset_msu_withcomments$comments[subset_msu_withcomments$Vaderposneg=='0'], main='Negative Sentiment', xlab='Number of Comments')
hist(subset_msu_withcomments$comments[subset_msu_withcomments$Vaderposneg=='1'], main='Positive Sentiment', xlab='Number of Comments')

t.test(subset_msu_withcomments$comments[subset_msu_withcomments$Vaderposneg=='0'],
      subset_msu_withcomments$comments[subset_msu_withcomments$Vaderposneg=='1'])
```

ANSWER: Even when removing posts that were not highly engaged with, the number of comments was not significantly different between posts with positive sentiments as compared with negative sentiment posts.